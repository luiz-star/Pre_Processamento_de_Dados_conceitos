{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pré_Processamento de Dados_conceitos.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luiz-star/Pre_Processamento_de_Dados_conceitos/blob/main/Pr%C3%A9_Processamento_de_Dados_conceitos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUwCbt07o7UC"
      },
      "source": [
        "# Especialização em Ciência de Dados - PUC-Rio\n",
        "## Machine Learning \n",
        "### Pré-Processamento de Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-d_toBSBjPX"
      },
      "source": [
        "**Referências:** https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1gTKd1qgRQg"
      },
      "source": [
        "# configuração para não exibir os warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Importação de pacotes\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler # para normalização\n",
        "from sklearn.preprocessing import StandardScaler # para padronização\n",
        "from sklearn.preprocessing import OrdinalEncoder # para ordinal encoding\n",
        "from sklearn.preprocessing import OneHotEncoder # para one-hot encoding e dummies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTVLPtqFqPfr"
      },
      "source": [
        "## Transformações Numéricas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PSf9bAxvJhR"
      },
      "source": [
        "Muitos algoritmos de machine learning apresentam um melhor desempenho quando os atributos de entrada numéricos são redimensionados (exemplos: algoritmos que trabalham com a soma ponderada de entradas, como regressão linear, regressão logística e redes neurais; e algoritmos baseados em distância ou produto interno dos atributos de entrada, como o KNN e SVM).\n",
        "\n",
        "As duas técnicas mais populares para redimensionar dados numéricos antes da modelagem são:\n",
        "\n",
        "* **Normalização:** dimensiona cada atributo separadamente entre 0 e 1\n",
        "* **Padronização:** dimensiona cada atributo separadamente subtraindo a média e dividindo pelo desvio padrão, transformando em uma distribuição normal padrão com média 0 e desvio padrão 1.\n",
        "\n",
        "Já sabemos que modelos de Machine Learning mapeiam os atributos de entrada em uma variável de saída, e que a escala e a distribuição dos dados podem ser diferentes para cada atributo do dataset, bem como terem diferentes unidades.\n",
        "\n",
        "Estas diferenças podem aumentar a dificuldade do problema a ser modelado. Além disso, atributos de entrada com valores muito grandes podem resultar em um modelo que aprende pesos com valores altos, o que pode tornar o modelo instável e com performance de aprendizado ruim, pois pode ser muito sensível às diferenças dos valores de entrada, podendo resultar em um alto erro de generalização.\n",
        "\n",
        "A diferença na escala dos atributos não afeta todos os algoritmos de machine learning. Geralmente, os algoritmos baseados em árvores (árvores de decisão ou random forest) não são afetados.\n",
        "\n",
        "*Assunto de MLA: Em redes neurais, também pode ser interessante redimensionar a variável de saída de problemas de regressão, para tornar o aprendizado mais fácil (devido à questão do erro do gradiente descendente: variáveis de saída muito grandes fazem com que os pesos se modifiquem muito bruscamente, tornando o processo de aprendizado instável). Ao final do aprendizado, na aplicação do modelo, a variável de saída pode ser pós-processada, para que seja informado o valor de saída real.*\n",
        "\n",
        "Podemos utilizar as operação de normalização e padronização usando a biblioteca **Scikit-learn**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHhKXN6dpKdW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7bae4ba-d2e5-4cd3-abc8-2d6a945f8e29"
      },
      "source": [
        "# dados que iremos usar nos exemplos\n",
        "data = np.asarray([[100, 0.001],\n",
        "\t\t\t\t[8, 0.05],\n",
        "\t\t\t\t[50, 0.005],\n",
        "\t\t\t\t[88, 0.07],\n",
        "\t\t\t\t[4, 0.1]])\n",
        "print(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.0e+02 1.0e-03]\n",
            " [8.0e+00 5.0e-02]\n",
            " [5.0e+01 5.0e-03]\n",
            " [8.8e+01 7.0e-02]\n",
            " [4.0e+00 1.0e-01]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZbuLX5Sp1pX"
      },
      "source": [
        "### Normalização"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBVVs-8vu8JC"
      },
      "source": [
        "A Normalização redimensiona os dados do intervalo original para um novo intervalo entre 0 e 1. São utilizados os valores mínimo e máximo observáveis, sendo possível estimar esses valores a partir dos dados disponíveis.\n",
        "\n",
        "Notmalizamos os dados dividindo todos os valores pelo valor máximo encontrado ou subtraindo o valor mínimo e dividindo pelo intervalo entre os valores máximo e mínimo:\n",
        "\n",
        "*y = (x – min) / (max – min)*\n",
        "\n",
        "Podemos normalizar os dados usando o objeto **MinMaxScaler**, do pacote **Scikit-learn**. A escala padrão é o intervalo [0,1], mas é possível especificar outro intervalo através do parâmetro *feature_range*, que será utilizado para todas as variáveis normalizadas.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7hGUQHIpWce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aba22aa9-d5af-4f62-9cd6-c8e96b1eac86"
      },
      "source": [
        "# definindo o transformador como min max scaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# transformando os dados\n",
        "scaled = scaler.fit_transform(data)\n",
        "print(scaled)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.         0.        ]\n",
            " [0.04166667 0.49494949]\n",
            " [0.47916667 0.04040404]\n",
            " [0.875      0.6969697 ]\n",
            " [0.         1.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meF1JOIaqRmO"
      },
      "source": [
        "### Padronização"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAx2IyEVvefx"
      },
      "source": [
        "A Padronização redimensiona a distribuição dos valores observados para que a sua média seja 0 e o seu desvio padrão 1, também conhecida como subtração do valor médio ou centralização dos dados.\n",
        "\n",
        "A padronização pressupõe que suas observações sigam uma distribuição Normal. Ainda é possível padronizar seus dados mesmo se essa premissa não for verdadeira, mas talvez os resultados sejam prejudicados.\n",
        "\n",
        "Para padronizar os dados, calcula-se a média estatística e o desvio padrão dos valores dos atributos, subtrai-se a média de cada valor e divide-se o resultado pelo desvio padrão. \n",
        "\n",
        "A padronização exige que consigamos estimar com precisão a média e o desvio padrão dos valores observáveis. Recomenda-se estimar esses valores com base nos dados de treinamento, não no conjunto de dados inteiro.\n",
        "\n",
        "Um valor é padronizado da seguinte maneira:\n",
        "\n",
        "* média = soma (x) / contagem (x)\n",
        "* desvio_ padrão = sqrt (soma ((x - média) ^ 2) / contagem (x))\n",
        "* y = (x - média) / desvio padrão\n",
        "\n",
        "Podemos padronizar os dados usando o objeto **StandardScaler**, do pacote **Scikit-learn**.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CelMP4Npy4r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fc6453c-e7b2-4a32-9791-3374e88e0ac4"
      },
      "source": [
        "# definindo o transformador como standard scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# transformando os dados\n",
        "scaled = scaler.fit_transform(data)\n",
        "print(scaled)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1.26398112 -1.16389967]\n",
            " [-1.06174414  0.12639634]\n",
            " [ 0.         -1.05856939]\n",
            " [ 0.96062565  0.65304778]\n",
            " [-1.16286263  1.44302493]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhSyaQq6ygMe"
      },
      "source": [
        "**Quando Normalizar e quando Padronizar?**\n",
        "\n",
        "* Se a distribuição é normal, padronize. Caso contrário, normalize.\n",
        "* Os problemas de modelagem preditiva são muitas vezes complexos, não sendo clara a melhor transformação para realizar.\n",
        "* Na dúvida, use a normalização. Se tiver tempo, explore os modelos com os dados sem transformação, com a padronização e com a normalização e veja se os resultados são significativamente diferentes e se o custo x benefício vale a pena.\n",
        "* Como a padronização resulta em valores positivos e negativos, pode ser interessante normalizar os dados após a padronização.\n",
        "* É possível definir os valores de mínimo e máximo de acordo com o conhecimento no negócio (e não simplesmente se ater aos valores observados)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuj3mHe4qUPO"
      },
      "source": [
        "## Transformações Categóricas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCPjwhG42FjU"
      },
      "source": [
        "Algumas implementações de modelos de Machine learning requerem que os atributos sejam numéricos, sendo necessário codificar os atributos categóricos em numéricos antes de treinar e utilizar o modelo.\n",
        "\n",
        "Usamos o **ordinal encoding** para as variáveis categóricas ordinais e o **one-hot encoding** para as variáveis categóricas nominais.\n",
        "\n",
        "OBS: em alguns lugares, se usa \"variáveis nominais\" como sinônimo de \"variáveis categóricas\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKvm7cg6qa9g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f13755ff-65ba-41e2-e0ab-46f8b417ddc8"
      },
      "source": [
        "# dados que iremos usar nos exemplos\n",
        "data = np.asarray([['red'], ['green'], ['blue']])\n",
        "print(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['red']\n",
            " ['green']\n",
            " ['blue']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E47UeFzUrLqg"
      },
      "source": [
        "### Ordinal Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-k05diRg2-ni"
      },
      "source": [
        "No *ordinal encoding*, cada categoria única é transformada em um número inteiro, mantendo o relacionamento ordinal entre as variáveis. Este encoding não deve ser utilizado quando a variável não é ordinal, pois seria criada uma ordenação que não existe.\n",
        "\n",
        "O *ordinal encoding* é implementado pelo objeto **OrdinalEncoder**, do pacote **Scikit-learn**. Por padrão, serão atribuídos inteiros aos valores categóricos ordinais em ordem alfabética, mas é possível especificar a ordenação desejada através do parâmetro *categories*.\n",
        "\n",
        "A classe OrdinalEncoder é urtilizada com atributos em formato matricial (organizadas em linhas e colunas). Se quisermos codificar uma variável target categórica, devemos usar a classe **LabelEncoder**, similar ao **OrdinalEncoder**, mas que espera um input de 1 dimensão apenas.\n",
        "\n",
        "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcxsG9YeqbDo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bb69bdd-e984-44c1-beda-88fb75111809"
      },
      "source": [
        "# definindo o transformador como ordinal encoding\n",
        "encoder = OrdinalEncoder()\n",
        "\n",
        "# transformando os dados\n",
        "result = encoder.fit_transform(data)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2.]\n",
            " [1.]\n",
            " [0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDfZ7IbPrOYN"
      },
      "source": [
        "### One Hot Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGLHFKo84_aF"
      },
      "source": [
        "Para variáveis categóricas nominais, sem ordenação existente entre elas, devemos utilizar o one-hot encoding. Em vez de uma variável inteira, é criada uma variável binária para cada valor único da variável. Este nome se dá porque para cada possível categoria, apenas um bit é \"ativado\".\n",
        "\n",
        "O one-hot encoding é implementado pelo objeto **OneHotEncoder**, do pacote **Scikit-learn**, que ordena as categorias alfabeticamente antes de aplicar a transformação. É possível especificar a lista de categorias através do parâmetro *categories*.\n",
        "\n",
        "Espera-se que o conjunto de treinamento contenha pelo menos um exemplo de cada categoria se as categorias não forem explicitamente definidas. Se os novos dados (conjunto de teste, por exemplo) tiverem categorias não vistas no treinamento, é possível configurar o parâmetro \"handle_unknown\" como \"ignore\" para que não ocorra um erro.\n",
        "\n",
        "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRRAHCcxqzEN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3520b3c6-1bb5-413b-f01a-5b11afc1a6f7"
      },
      "source": [
        "# definindo o transformador como one hot encoding\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "# transformando os dados\n",
        "onehot = encoder.fit_transform(data)\n",
        "print(onehot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eDpdjYWrR0x"
      },
      "source": [
        "### Dummy Variable Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3f_usxt7O1z"
      },
      "source": [
        "O one-hot encoding cria uma variável binária para cada categoria, mas esta representação inclui redundância. Por exemplo, se soubermos que [1, 0, 0] representa \"azul\" e [0, 1, 0] representa \"verde\", não precisamos de outra variável binária [0, 0, 1] para representar \"vermelho\". Poderíamos usar [0, 0] para \"vermelho\", [1, 0] para \"azul\" e [0, 1] para \"verde\". A codificação de variável dummy representa C categorias em C-1 variáveis ​​binárias.\n",
        "\n",
        "> **Nota de nível avançado:** Em modelos de regresão (que têm termo de viés - bias) é necesário optar pelo dummy encoding no lugar do one-hot encoding, pois o one-hot encoding fará com que a matriz de dados de entrada seja singular (não-inversível), impossibilitando o cálculo dos coeficientes de regressão (que usa operações de álgebra linear). Saiba mais em: https://towardsdatascience.com/one-hot-encoding-multicollinearity-and-the-dummy-variable-trap-b5840be3c41a e https://inmachineswetrust.com/posts/drop-first-columns/\n",
        "\n",
        "A codificação dummy também é implementado pelo objeto **OneHotEncoder**, do pacote **Scikit-learn**, usando o parâmetro *drop* para indicar qual categoria receberá todos os valores zero, chamada de \"linha de base\". Podemos usar *first* para que a primeira categoria seja usada (as categorias são ordenadas alfabeticamente)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vA_bmO8q85I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae10e356-34bb-4b08-88e3-59cf94356b42"
      },
      "source": [
        "# definindo o transformador como one hot encoding (com Dummy variable encoder)\n",
        "encoder = OneHotEncoder(drop='first', sparse=False)\n",
        "\n",
        "# transformando os dados\n",
        "onehot = encoder.fit_transform(data)\n",
        "print(onehot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 1.]\n",
            " [1. 0.]\n",
            " [0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "To9jCkxSFZ8v"
      },
      "source": [
        "Se tivermos no mesmo datsset atributos dos dois tipos (numéricos e categóricos), será necessário transformar/codificar cada atributo (coluna) separadamente e concatenar todas as variáveis ​​preparadas novamente em uma única matriz para ajustar ou avaliar o modelo (ou usar o **ColumnTransformer** para aplicar condicionalmente diferentes transformações de dados a diferentes variáveis ​​de entrada).\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqIN4FR7iolQ"
      },
      "source": [
        "## Exercícios"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApT23HyVkDzV"
      },
      "source": [
        "# configuração para não exibir os warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Importação de pacotes\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler # para normalização\n",
        "from sklearn.preprocessing import StandardScaler # para padronização"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xknWmbkj-xN"
      },
      "source": [
        "# Carrega arquivo csv usando Pandas usando uma URL\n",
        "\n",
        "# Informa a URL de importação do dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "\n",
        "# Informa o cabeçalho das colunas\n",
        "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "\n",
        "# Lê o arquivo utilizando as colunas informadas\n",
        "dataset = pd.read_csv(url, names=colunas, skiprows=0, delimiter=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcMT9DilkUww"
      },
      "source": [
        "# Pegando apenas os dados do dataset e guardando em um array\n",
        "array = dataset.values\n",
        "\n",
        "# Separando o array em componentes de input e output\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwQNT5MPkVe1"
      },
      "source": [
        "# 1. Normalize o dataset usando MinMaxScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sk9Pei18kbcy"
      },
      "source": [
        "# 2. Padronize o dataset usando StandardScaler "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TkjmDL0kwAi"
      },
      "source": [
        "## Gabarito"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBuTIcbGk1xS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10dc2122-cd63-4290-d433-6bc7205e1d86"
      },
      "source": [
        "# 1. Normalize o dataset usando MinMaxScaler\n",
        "\n",
        "# Normalizando os dados\n",
        "scaler = MinMaxScaler().fit(X)\n",
        "normalizedX = scaler.transform(X)\n",
        "\n",
        "# Sumarizando os dados transformados\n",
        "print(\"Dados Originais: \\n\\n\", dataset.values)\n",
        "print(\"\\nDados Normalizados: \\n\\n\", normalizedX[0:5,:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dados Originais: \n",
            "\n",
            " [[  6.    148.     72.    ...   0.627  50.      1.   ]\n",
            " [  1.     85.     66.    ...   0.351  31.      0.   ]\n",
            " [  8.    183.     64.    ...   0.672  32.      1.   ]\n",
            " ...\n",
            " [  5.    121.     72.    ...   0.245  30.      0.   ]\n",
            " [  1.    126.     60.    ...   0.349  47.      1.   ]\n",
            " [  1.     93.     70.    ...   0.315  23.      0.   ]]\n",
            "\n",
            "Dados Normalizados: \n",
            "\n",
            " [[0.35294118 0.74371859 0.59016393 0.35353535 0.         0.50074516\n",
            "  0.23441503 0.48333333]\n",
            " [0.05882353 0.42713568 0.54098361 0.29292929 0.         0.39642325\n",
            "  0.11656704 0.16666667]\n",
            " [0.47058824 0.91959799 0.52459016 0.         0.         0.34724292\n",
            "  0.25362938 0.18333333]\n",
            " [0.05882353 0.44723618 0.54098361 0.23232323 0.11111111 0.41877794\n",
            "  0.03800171 0.        ]\n",
            " [0.         0.68844221 0.32786885 0.35353535 0.19858156 0.64232489\n",
            "  0.94363792 0.2       ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTQhED9bk10M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ecdc606-23f0-479f-e89f-ee61ffabb837"
      },
      "source": [
        "# 2. Padronize o dataset usando StandardScaler \n",
        "\n",
        "# Padronizando os dados\n",
        "scaler = StandardScaler().fit(X)\n",
        "standardX = scaler.transform(X)\n",
        "\n",
        "# Sumarizando os dados transformados\n",
        "print(\"Dados Originais: \\n\\n\", dataset.values)\n",
        "print(\"\\nDados Padronizados: \\n\\n\", standardX[0:5,:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dados Originais: \n",
            "\n",
            " [[  6.    148.     72.    ...   0.627  50.      1.   ]\n",
            " [  1.     85.     66.    ...   0.351  31.      0.   ]\n",
            " [  8.    183.     64.    ...   0.672  32.      1.   ]\n",
            " ...\n",
            " [  5.    121.     72.    ...   0.245  30.      0.   ]\n",
            " [  1.    126.     60.    ...   0.349  47.      1.   ]\n",
            " [  1.     93.     70.    ...   0.315  23.      0.   ]]\n",
            "\n",
            "Dados Padronizados: \n",
            "\n",
            " [[ 0.63994726  0.84832379  0.14964075  0.90726993 -0.69289057  0.20401277\n",
            "   0.46849198  1.4259954 ]\n",
            " [-0.84488505 -1.12339636 -0.16054575  0.53090156 -0.69289057 -0.68442195\n",
            "  -0.36506078 -0.19067191]\n",
            " [ 1.23388019  1.94372388 -0.26394125 -1.28821221 -0.69289057 -1.10325546\n",
            "   0.60439732 -0.10558415]\n",
            " [-0.84488505 -0.99820778 -0.16054575  0.15453319  0.12330164 -0.49404308\n",
            "  -0.92076261 -1.04154944]\n",
            " [-1.14185152  0.5040552  -1.50468724  0.90726993  0.76583594  1.4097456\n",
            "   5.4849091  -0.0204964 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}